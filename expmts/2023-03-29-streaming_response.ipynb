{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: \"Streaming Response\"\n",
    "date:   2023-03-30\n",
    "tags: [Tech, AI]\n",
    "comments: true\n",
    "author: GingerNg\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming refers to the process of transferring data over a network in a continuous and real-time manner, rather than downloading the entire content before playback can begin. Streaming enables users to access and view or listen to content without the need for downloading or storing the entire file on their device.\n",
    "\n",
    "Streaming technology is used in a variety of applications, including video and audio streaming, live broadcasting, online gaming, and cloud computing. It allows for the real-time delivery of content to a user's device while the content is still being transmitted from a server or another source.\n",
    "\n",
    "Streaming is accomplished by dividing data into small packets and sending them in a continuous stream over a network. This allows the user to access and view the data in real-time, without having to wait for the entire file to download. Streaming can be done using a variety of protocols, including HTTP, RTSP, and P2P.\n",
    "\n",
    "The above content is generated by **ChatGPT**\n",
    "\n",
    "Recently, I'm using [OpenAI completion API](https://platform.openai.com/docs/api-reference/completions/create) which has a stream option. When set _stream=True_, api return a generator, tokens are sent as stream. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI API stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank\n",
      " you\n",
      ",\n",
      " I\n",
      " try\n",
      " my\n",
      " best\n",
      " to\n",
      " be\n",
      " helpful\n",
      " in\n",
      " any\n",
      " way\n",
      " I\n",
      " can\n",
      ".\n",
      " Is\n",
      " there\n",
      " anything\n",
      " specific\n",
      " I\n",
      " can\n",
      " assist\n",
      " you\n",
      " with\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [ {\"role\": \"user\", \"content\": \"You are a helpful assisant\"}]\n",
    "kwargs = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": messages,\n",
    "    \"timeout\": 5,\n",
    "    \"stream\": True,\n",
    "    \"presence_penalty\": 1,\n",
    "    # \"max_tokens\": 800,\n",
    "    \"temperature\": 0.8\n",
    "}\n",
    "response = openai.ChatCompletion.create(**kwargs)    \n",
    "for r in response:\n",
    "        one = r.choices[0].delta.content if 'content' in r.choices[0].delta else ''\n",
    "        print(one) # by this mode, it is easy to realize the typewriter effect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generator\n",
    "In python, generator is commonly used. The keyword **yield** will give the caller a generator. You can also get a generator from python **comprehension**(used by OpenAI SDK).\n",
    "Generator is a kind of iterator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x1243b34a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprehension\n",
    "myIterator = ( x*2 for x in range(5))\n",
    "myIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    print('starting')\n",
    "    # while True:  # 可重复迭代\n",
    "    for i in range(5):\n",
    "        r = yield i\n",
    "        # print(r)\n",
    "f = foo()\n",
    "for i in f:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Streaming Response of Web Framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottle supports streaming response by using **yield** in view function. This [post](https://blog.tonyseek.com/post/flask-stream-response/)[1] show examples of flask. We give a example of fastapi below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from fastapi.responses import StreamingResponse\n",
    "app = FastAPI()\n",
    "@app.get(\"/stream_demo\")  # 异步流式返回\n",
    "async def stream_response():\n",
    "    async def data_generator():\n",
    "        for i in range(10):\n",
    "            yield f\"Chunk {i}\\n\"\n",
    "            await asyncio.sleep(1)\n",
    "    dg = data_generator()\n",
    "    print(dg)\n",
    "    return StreamingResponse(dg, media_type=\"text/plain\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8082)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### requests\n",
    "set stream=True in requests.get(...) then headers['Transfer-Encoding'] = 'chunked' is set in the HTTP headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "b'Chunk 0\\n'\n",
      "b'Chunk 1\\n'\n",
      "b'Chunk 2\\n'\n",
      "b'Chunk 3\\n'\n",
      "b'Chunk 4\\n'\n",
      "b'Chunk 5\\n'\n",
      "b'Chunk 6\\n'\n",
      "b'Chunk 7\\n'\n",
      "b'Chunk 8\\n'\n",
      "b'Chunk 9\\n'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "resp = requests.get(\"http://localhost:8082/stream_demo\", stream=True)\n",
    "print(resp)\n",
    "for r in resp:\n",
    "    print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code can be found in [here](expmts/2023-03-29-streaming_response.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### references\n",
    "- [1] [在 Flask 里产生流式响应](https://blog.tonyseek.com/post/flask-stream-response/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
